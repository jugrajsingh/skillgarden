# GrepAI dev stack: Ollama (embeddings) + Qdrant (vector store)
#
# Usage:
#   docker compose up -d
#   docker exec ollama ollama pull bge-m3
#
# grepai workspace config:
#   embedder:
#     provider: ollama
#     model: bge-m3
#     endpoint: http://localhost:11434
#   store:
#     backend: qdrant
#     qdrant:
#       endpoint: http://localhost:6334
#
# Notes:
#   - No GPU support: Intel NPU is unsupported by Ollama, runs on CPU
#   - ollama/ollama:latest is the only official image (no smaller CPU-only variant)
#   - Models are persisted in the ollama_data volume across restarts
#   - Qdrant is used for workspace mode (cross-project search)
#   - Qdrant data is persisted in the qdrant_data volume

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    restart: unless-stopped

volumes:
  ollama_data:
  qdrant_data:
